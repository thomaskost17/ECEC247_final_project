{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module Initialization\n",
    "Here we will load all data and relevant modules. This will access helpful utilities aswell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.misc\n",
    "from scipy import signal\n",
    "from scipy.fftpack import fft, fftshift, ifft\n",
    "from scipy.fftpack import fftfreq\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from utils.utils import *\n",
    "from utils.preprocess import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from utils.utils import *\n",
    "from utils.preprocess import *\n",
    "from nn.rnn import *\n",
    "from nn.cnn import *\n",
    "from nn.cnn_lstm import *\n",
    "from nn.cnn_gru import *\n",
    "from nn.solver import *\n",
    "import torch, torchaudio, torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable \n",
    "import copy\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for macOS\n",
    "X_test = np.load(\"X_test.npy\")\n",
    "y_test = np.load(\"y_test.npy\")\n",
    "person_train_valid = np.load(\"person_train_valid.npy\")\n",
    "X_train_valid = np.load(\"X_train_valid.npy\")\n",
    "y_train_valid = np.load(\"y_train_valid.npy\")\n",
    "person_test = np.load(\"person_test.npy\")\n",
    "\n",
    "X_test_cpy =copy.deepcopy( X_test)\n",
    "y_test_cpy =copy.deepcopy( y_test)\n",
    "person_train_valid_cpy =copy.deepcopy( person_train_valid)\n",
    "X_train_valid_cpy =copy.deepcopy( X_train_valid)\n",
    "y_train_valid_cpy =copy.deepcopy( y_train_valid)\n",
    "person_test_cpy =copy.deepcopy( person_test)\n",
    "\n",
    "static_X_test = np.load(\"X_test.npy\")\n",
    "static_y_test = np.load(\"y_test.npy\")\n",
    "static_y_test -= 769\n",
    "static_X_train_valid = np.load(\"X_train_valid.npy\")\n",
    "static_y_train_valid = np.load(\"y_train_valid.npy\") \n",
    "static_y_train_valid -= 769\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If data in ./data/project\n",
    "X_test = np.load(add_path(\"X_test.npy\"))\n",
    "y_test = np.load(add_path(\"y_test.npy\"))\n",
    "person_train_valid = np.load(add_path(\"person_train_valid.npy\"))\n",
    "X_train_valid = np.load(add_path(\"X_train_valid.npy\"))\n",
    "y_train_valid = np.load(add_path(\"y_train_valid.npy\"))\n",
    "person_test = np.load(add_path(\"person_test.npy\"))\n",
    "X_test_cpy =copy.deepcopy( X_test)\n",
    "y_test_cpy =copy.deepcopy( y_test)\n",
    "person_train_valid_cpy =copy.deepcopy( person_train_valid)\n",
    "X_train_valid_cpy =copy.deepcopy( X_train_valid)\n",
    "y_train_valid_cpy =copy.deepcopy( y_train_valid)\n",
    "person_test_cpy =copy.deepcopy( person_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the shape of our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Training/Valid data shape: {}'.format(X_train_valid.shape))\n",
    "print('Test data shape: {}'.format(X_test.shape))\n",
    "print('Training/Valid Target shape: {}'.format(y_train_valid.shape))\n",
    "print('Test Target shape: {}'.format(y_test.shape))\n",
    "print('Person Train/Valid shape: {}'.format(person_train_valid.shape))\n",
    "print('Person Test shape: {}'.format(person_test.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "Here we will perfrom some data preprocessing to create a larger dataset and improve the generalization of our network.\n",
    "\n",
    "## Outline\n",
    "**Describe the preprocessing that will be done**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS CELL ALONE WILL DO ALL PREPROCESSING\n",
    "# Do all preprocessing and pair patient labels\n",
    "person_test,x_test,y_test,person_valid,x_valid,y_valid,person_train, x_train, y_train = std_preprocess_EEG(\n",
    "    X_test = X_test,\n",
    "    y_test = y_test,\n",
    "    person_train_valid = person_train_valid,\n",
    "    X_train_valid = X_train_valid,\n",
    "    y_train_valid = y_train_valid,\n",
    "    person_test = person_test,\n",
    "    val_size = 5000)\n",
    "print('Training data shape: {}'.format(x_train.shape))\n",
    "print('Validation data shape: {}'.format(x_valid.shape))\n",
    "print('Test data shape: {}'.format(x_test.shape))\n",
    "print('Training Target shape: {}'.format(y_train.shape))\n",
    "print('Validation Target shape: {}'.format(y_valid.shape))\n",
    "print('Test Target shape: {}'.format(y_test.shape))\n",
    "print('Person Train shape: {}'.format(person_train.shape))\n",
    "print('Person Validation shape: {}'.format(person_valid.shape))\n",
    "print('Person Test shape: {}'.format(person_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_train_valid -= 769\n",
    "#y_test -= 769\n",
    "\n",
    "## Visualizing the data\n",
    "\n",
    "ch_data = X_train_valid[:,8,:]\n",
    "\n",
    "\n",
    "class_0_ind = np.where(y_train_valid == 0)\n",
    "ch_data_class_0 = ch_data[class_0_ind]\n",
    "avg_ch_data_class_0 = np.mean(ch_data_class_0,axis=0)\n",
    "\n",
    "\n",
    "class_1_ind = np.where(y_train_valid == 1)\n",
    "ch_data_class_1 = ch_data[class_1_ind]\n",
    "avg_ch_data_class_1 = np.mean(ch_data_class_1,axis=0)\n",
    "\n",
    "class_2_ind = np.where(y_train_valid == 2)\n",
    "ch_data_class_2 = ch_data[class_2_ind]\n",
    "avg_ch_data_class_2 = np.mean(ch_data_class_2,axis=0)\n",
    "\n",
    "class_3_ind = np.where(y_train_valid == 3)\n",
    "ch_data_class_3 = ch_data[class_3_ind]\n",
    "avg_ch_data_class_3 = np.mean(ch_data_class_3,axis=0)\n",
    "\n",
    "\n",
    "plt.plot(np.arange(1000),avg_ch_data_class_0)\n",
    "plt.plot(np.arange(1000),avg_ch_data_class_1)\n",
    "plt.plot(np.arange(1000),avg_ch_data_class_2)\n",
    "plt.plot(np.arange(1000),avg_ch_data_class_3)\n",
    "plt.axvline(x=500, label='line at t=500',c='cyan')\n",
    "\n",
    "plt.legend([\"Cue Onset left\", \"Cue Onset right\", \"Cue onset foot\", \"Cue onset tongue\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_filter(X_train_valid,X_test):\n",
    "    num_fft=500\n",
    "    X_train_valid_denoise=np.zeros(X_train_valid.shape)\n",
    "    X_test_denoise=np.zeros(X_test.shape)\n",
    "#define filter\n",
    "    filter_denoise=scipy.signal.firwin(1001,[0.4,0.48],pass_zero='bandstop')\n",
    "    for i in range(X_train_valid.shape[1]):\n",
    "        for j in range(X_train_valid.shape[0]):\n",
    "            X_train_valid_denoise[j,i,:]=scipy.signal.convolve(X_train_valid[j,i,:],filter_denoise,mode='same')\n",
    "    for i in range(X_test.shape[1]):\n",
    "        for j in range(X_test.shape[0]):\n",
    "            X_test_denoise[j,i,:]=scipy.signal.convolve(X_test[j,i,:],filter_denoise,mode='same')\n",
    "    return X_train_valid_denoise,X_test_denoise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_valid_denoise,X_test_denoise = data_filter(X_train_valid_cpy, X_test_cpy)\n",
    "print('Training/Valid data shape: {}'.format(X_train_valid_denoise.shape))\n",
    "print('Test data shape: {}'.format(X_test_denoise.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do all preprocessing and pair patient labels\n",
    "person_test_den,x_test_den,y_test_den,person_valid_den,x_valid_den,y_valid_den,person_train_den, x_train_den, y_train_den= std_preprocess_EEG(\n",
    "    X_test = X_test_denoise,\n",
    "    y_test = y_test_cpy,\n",
    "    person_train_valid = person_train_valid_cpy,\n",
    "    X_train_valid = X_train_valid_denoise,\n",
    "    y_train_valid = y_train_valid_cpy,\n",
    "    person_test = person_test_cpy,\n",
    "    val_size = 5000)\n",
    "print('Denoised Training data shape: {}'.format(x_train_den.shape))\n",
    "print('Denoised Validation data shape: {}'.format(x_valid_den.shape))\n",
    "print('Denoised Test data shape: {}'.format(x_test_den.shape))\n",
    "print('Denoised Training Target shape: {}'.format(y_train_den.shape))\n",
    "print('Denoised Validation Target shape: {}'.format(y_valid_den.shape))\n",
    "print('Denoised Test Target shape: {}'.format(y_test_den.shape))\n",
    "print('Denoised Person Train shape: {}'.format(person_train_den.shape))\n",
    "print('Denoised Person Validation shape: {}'.format(person_valid_den.shape))\n",
    "print('Denoised Person Test shape: {}'.format(person_test_den.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_valid_prep,y_train_valid_prep = data_prep(X_train_valid,y_train_valid,2,2,True, 0, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to_categorical is a tensorflow command so here it is in python\n",
    "def to_categorical(y, num_classes):\n",
    "    \"\"\" 1-hot encodes a tensor \"\"\"\n",
    "    return np.eye(num_classes, dtype='uint8')[y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Possible augmentations\n",
    "# Create Data set with gaussian noise\n",
    "std_data = np.std(X_train_valid)\n",
    "X_train_valid_gn = np.random.randn(*X_train_valid.shape)*std_data/10 +X_train_valid\n",
    "\n",
    "# Create data set with single sample shift\n",
    "X_train_valid_delay = np.roll(X_train_valid,1,axis=2)\n",
    "\n",
    "# Create data set that is scaled \n",
    "X_train_valid_half = X_train_valid/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning\n",
    "\n",
    "In this section, we tuned our hyperparameters for each network to achieve best performance. We have commented out the hyperparameters that were tested out for each network.\n",
    "\n",
    "## CNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50 #epochs tested: 3, 5, 10, 20, 30, 50, 60\n",
    "learning_rate = 0.001 #lrs tested: 0.01, 0.001, 0.0001,0.00001\n",
    "\n",
    "# hyperparameters for LSTM\n",
    "input_size = 250 #number of features \n",
    "hidden_size = 128 #number of features in hidden state\n",
    "num_layers = 1 #number of stacked lstm layers \n",
    "\n",
    "# class and batch size\n",
    "num_classes = 4 #number of output classes \n",
    "batch_size = 64 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Torch Iterables\n",
    "train_dataset = torch.utils.data.TensorDataset(x_train, y_train)\n",
    "valid_dataset = torch.utils.data.TensorDataset(x_valid, y_valid)\n",
    "test_dataset  = torch.utils.data.TensorDataset(x_test, y_test)\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cnn = CNN(num_classes, input_size, hidden_size, num_layers, x_train.shape[1])\n",
    "criterion = torch.nn.CrossEntropyLoss()   # CE Loss is our softmax\n",
    "optimizer = torch.optim.Adam(cnn.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9) #lr decays tested: 0.9, 0.95, 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solver = Solver(num_epochs, cnn, optimizer, scheduler, criterion, verbose=True )\n",
    "solver.train(trainloader, validloader)\n",
    "solver.test(testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(3, 1, 1)\n",
    "plt.title('Training and validation loss for CNN')\n",
    "plt.xlabel('Epoch')\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(solver.loss_history, label='training')\n",
    "plt.plot(solver.val_loss_history, label='validation')\n",
    "plt.legend([\"train\", \"val\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50 #epochs tested: 3, 5, 10, 20, 30, 50, 60\n",
    "learning_rate = 0.0001 #lrs tested: 0.01, 0.001, 0.0001,0.00001\n",
    "\n",
    "# hyperparameters for LSTM\n",
    "input_size = 250 #number of features tested: 250, 1000\n",
    "hidden_size = 128 #number of features in hidden state tested: 16,32,64,128,250\n",
    "num_layers = 1 #number of stacked lstm layers tested: 1,2,4,6\n",
    "\n",
    "# class and batch size\n",
    "num_classes = 4 #number of output classes \n",
    "batch_size = 64 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = RNN(num_classes, input_size, hidden_size, num_layers, x_train.shape[1])\n",
    "criterion = torch.nn.CrossEntropyLoss()   # CE Loss is our softmax\n",
    "optimizer = torch.optim.Adam(lstm.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9) #lr decays tested: 0.9, 0.95, 0.99\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "solver = Solver(num_epochs, lstm, optimizer, scheduler, criterion, verbose=True,cnn_reshape=False)\n",
    "solver.train(trainloader, validloader)\n",
    "solver.test(testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(3, 1, 1)\n",
    "plt.title('Training and validation loss for LSTM')\n",
    "plt.xlabel('Epoch')\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(solver.loss_history, label='training')\n",
    "plt.plot(solver.val_loss_history, label='validation')\n",
    "plt.legend([\"train\", \"val\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN_LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50 #epochs tested: 3, 5, 10, 20, 30, 50, 60\n",
    "learning_rate = 0.001 #lrs tested: 0.01, 0.001, 0.0001,0.00001\n",
    "\n",
    "# hyperparameters for LSTM\n",
    "input_size = 250 #number of features tested: 250, 1000\n",
    "hidden_size = 128 #number of features in hidden state tested: 16,32,64,128,250\n",
    "num_layers = 1 #number of stacked lstm layers tested: 1,2,4,6\n",
    "\n",
    "# class and batch size\n",
    "num_classes = 4 #number of output classes \n",
    "batch_size = 64 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_lstm = CNN_LSTM(num_classes, input_size, hidden_size, num_layers, x_train.shape[1])\n",
    "criterion = torch.nn.CrossEntropyLoss()   # CE Loss is our softmax\n",
    "optimizer = torch.optim.Adam(cnn_lstm.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9) #lr decays tested: 0.9, 0.95, 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "solver = Solver(num_epochs, cnn_lstm, optimizer, scheduler, criterion, verbose=True)\n",
    "solver.train(trainloader, validloader)\n",
    "solver.test(testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(3, 1, 1)\n",
    "plt.title('Training and validation loss for CNN-LSTM')\n",
    "plt.xlabel('Epoch')\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(solver.loss_history, label='training')\n",
    "plt.plot(solver.val_loss_history, label='validation')\n",
    "plt.legend([\"train\", \"val\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Subject Classification\n",
    "\n",
    "For this objective we are looking at how a model performs when trained on the dataset of each subject \n",
    "\n",
    "We test each model (RNN LSTM, CNN, CNN + LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set new parameters here for single subject classification\n",
    "num_epochs = 50 #epochs \n",
    "learning_rate = 0.001 #lr\n",
    "\n",
    "input_size = 250 #number of features \n",
    "hidden_size = 128 #number of features in hidden state\n",
    "num_layers = 1 #number of stacked lstm layers \n",
    "\n",
    "num_classes = 4 #number of output classes \n",
    "batch_size = 64 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Subject Classification with CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train CNN NETWORK on each subject and see how the model for each subject \n",
    "# with test dataset from each subject\n",
    "totalsub = 9\n",
    "subject_validaccs = []\n",
    "subject_testaccs = []\n",
    "for sub in range(0, totalsub):\n",
    "    train_dataset = torch.utils.data.TensorDataset(x_train_den[np.where(person_train_den == sub)[0]], y_train_den[np.where(person_train_den == sub)[0]])\n",
    "    valid_dataset = torch.utils.data.TensorDataset(x_valid_den[np.where(person_valid_den == sub)[0]], y_valid_den[np.where(person_valid_den == sub)[0]])\n",
    "    test_dataset  = torch.utils.data.TensorDataset(x_test_den[np.where(person_test_den == sub)[0]], y_test_den[np.where(person_test_den == sub)[0]])\n",
    "    trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "    validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "    testloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "    model = CNN(num_classes, input_size, hidden_size, num_layers, x_train.shape[1])\n",
    "    criterion = torch.nn.CrossEntropyLoss()   # CE Loss is our softmax\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "    solver = Solver(num_epochs, model, optimizer, scheduler, criterion, verbose=True )\n",
    "    solver.train(trainloader, validloader)\n",
    "    subject_validaccs.append(solver.val_accuracy_history[-1])\n",
    "    print(\"subject \" + str(sub+1) + \" validation accuracy: %1.5f\"% (solver.val_accuracy_history[-1]))\n",
    "    solver.test(testloader)\n",
    "    subject_testaccs.append(solver.test_accuracy)\n",
    "    print(\"subject \" + str(sub+1) + \" test accuracy: %1.5f\"% (solver.test_accuracy))\n",
    "    \n",
    "    plt.subplot(3, 1, 1)\n",
    "    plt.title('Training and validation loss per subject')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.subplot(3, 1, 1)\n",
    "    plt.plot(solver.loss_history, label='training')\n",
    "    plt.plot(solver.val_loss_history, label='validation')\n",
    "    plt.legend([\"train\", \"val\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(3, 1, 1)\n",
    "plt.title('Final validation and test accuracy per subject using CNN model')\n",
    "plt.ylabel('Model accuracy')\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(subject_validaccs, label='val')\n",
    "plt.plot(subject_testaccs, label='test')\n",
    "plt.legend([\"val\", \"test\"])\n",
    "plt.subplot(3, 1, 2)\n",
    "subjectnum = range(0, totalsub)\n",
    "plt.scatter(subjectnum, subject_validaccs)\n",
    "plt.scatter(subjectnum, subject_testaccs)\n",
    "plt.xlabel('Subject')\n",
    "plt.ylabel('Model accuracy')\n",
    "plt.legend([\"val\", \"test\"])\n",
    "\n",
    "cnn_subject_validaccs = subject_validaccs.copy()\n",
    "cnn_subject_testaccs = subject_testaccs.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Subject Classification with LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Change to optimal learning rate of 0.0001 for LSTM\n",
    "learning_rate = 0.0001 #lr\n",
    "\n",
    "# Train RNN LSTM NETWORK on each subject and see how the model for each subject \n",
    "# with test dataset from each subject\n",
    "totalsub = 9\n",
    "subject_validaccs = []\n",
    "subject_testaccs = []\n",
    "for sub in range(0, totalsub):\n",
    "    train_dataset = torch.utils.data.TensorDataset(x_train_den[np.where(person_train_den == sub)[0]], y_train_den[np.where(person_train_den == sub)[0]])\n",
    "    valid_dataset = torch.utils.data.TensorDataset(x_valid_den[np.where(person_valid_den == sub)[0]], y_valid_den[np.where(person_valid_den == sub)[0]])\n",
    "    test_dataset  = torch.utils.data.TensorDataset(x_test_den[np.where(person_test_den == sub)[0]], y_test_den[np.where(person_test_den == sub)[0]])\n",
    "    trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "    validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "    testloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "    model = RNN(num_classes, input_size, hidden_size, num_layers, x_train.shape[1])\n",
    "    criterion = torch.nn.CrossEntropyLoss()   # CE Loss is our softmax\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "    solver = Solver(num_epochs, model, optimizer, scheduler, criterion, verbose=True, cnn_reshape=False)\n",
    "    solver.train(trainloader, validloader)\n",
    "    subject_validaccs.append(solver.val_accuracy_history[-1])\n",
    "    print(\"subject \" + str(sub+1) + \" validation accuracy: %1.5f\"% (solver.val_accuracy_history[-1]))\n",
    "    solver.test(testloader)\n",
    "    subject_testaccs.append(solver.test_accuracy)\n",
    "    print(\"subject \" + str(sub+1) + \" test accuracy: %1.5f\"% (solver.test_accuracy))\n",
    "    \n",
    "    plt.subplot(3, 1, 1)\n",
    "    plt.title('Training and validation loss per subject')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.subplot(3, 1, 1)\n",
    "    plt.plot(solver.loss_history, label='training')\n",
    "    plt.plot(solver.val_loss_history, label='validation')\n",
    "    plt.legend([\"train\", \"val\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(3, 1, 1)\n",
    "plt.title('Final validation and test accuracy per subject using LSTM model')\n",
    "plt.ylabel('Model accuracy')\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(subject_validaccs, label='val')\n",
    "plt.plot(subject_testaccs, label='test')\n",
    "plt.legend([\"val\", \"test\"])\n",
    "plt.subplot(3, 1, 2)\n",
    "subjectnum = range(0, totalsub)\n",
    "plt.scatter(subjectnum, subject_validaccs)\n",
    "plt.scatter(subjectnum, subject_testaccs)\n",
    "plt.xlabel('Subject')\n",
    "plt.ylabel('Model accuracy')\n",
    "plt.legend([\"val\", \"test\"])\n",
    "\n",
    "\n",
    "lstm_subject_validaccs = subject_validaccs.copy()\n",
    "lstm_subject_testaccs = subject_testaccs.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Subject Classification with CNN-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Change to optimal learning rate for CNN-LSTM\n",
    "learning_rate = 0.001 #lr\n",
    "\n",
    "# Train CNN + LSTM NETWORK on each subject and see how the model for each subject \n",
    "# with test dataset from each subject\n",
    "totalsub = 9\n",
    "subject_validaccs = []\n",
    "subject_testaccs = []\n",
    "for sub in range(0, totalsub):\n",
    "    train_dataset = torch.utils.data.TensorDataset(x_train_den[np.where(person_train_den == sub)[0]], y_train_den[np.where(person_train_den == sub)[0]])\n",
    "    valid_dataset = torch.utils.data.TensorDataset(x_valid_den[np.where(person_valid_den == sub)[0]], y_valid_den[np.where(person_valid_den == sub)[0]])\n",
    "    test_dataset  = torch.utils.data.TensorDataset(x_test_den[np.where(person_test_den == sub)[0]], y_test_den[np.where(person_test_den == sub)[0]])\n",
    "    trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "    validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "    testloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "    model = CNN_LSTM(num_classes, input_size, hidden_size, num_layers, x_train.shape[1])\n",
    "    criterion = torch.nn.CrossEntropyLoss()   # CE Loss is our softmax\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "    solver = Solver(num_epochs, model, optimizer, scheduler, criterion, verbose=True)\n",
    "    solver.train(trainloader, validloader)\n",
    "    subject_validaccs.append(solver.val_accuracy_history[-1])\n",
    "    print(\"subject \" + str(sub+1) + \" validation accuracy: %1.5f\"% (solver.val_accuracy_history[-1]))\n",
    "    solver.test(testloader)\n",
    "    subject_testaccs.append(solver.test_accuracy)\n",
    "    print(\"subject \" + str(sub+1) + \" test accuracy: %1.5f\"% (solver.test_accuracy))\n",
    "    \n",
    "    plt.subplot(3, 1, 1)\n",
    "    plt.title('Training and validation loss per subject')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.subplot(3, 1, 1)\n",
    "    plt.plot(solver.loss_history, label='training')\n",
    "    plt.plot(solver.val_loss_history, label='validation')\n",
    "    plt.legend([\"train\", \"val\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(3, 1, 1)\n",
    "plt.title('Final validation and test accuracy per subject using CNN + LSTM model')\n",
    "plt.ylabel('Model accuracy')\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(subject_validaccs, label='val')\n",
    "plt.plot(subject_testaccs, label='test')\n",
    "plt.legend([\"val\", \"test\"])\n",
    "plt.subplot(3, 1, 2)\n",
    "subjectnum = range(0, totalsub)\n",
    "plt.scatter(subjectnum, subject_validaccs)\n",
    "plt.scatter(subjectnum, subject_testaccs)\n",
    "plt.xlabel('Subject')\n",
    "plt.ylabel('Model accuracy')\n",
    "plt.legend([\"val\", \"test\"])\n",
    "\n",
    "cnnlstm_subject_validaccs = subject_validaccs.copy()\n",
    "cnnlstm_subject_testaccs = subject_testaccs.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cnn_subject_validaccs)\n",
    "print(cnn_subject_testaccs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lstm_subject_validaccs)\n",
    "print(lstm_subject_testaccs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cnnlstm_subject_validaccs)\n",
    "print(cnnlstm_subject_testaccs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the classification accuracy of a single subject, in this case subject 1 (ID 0), the CNN model achieves the best validation and test accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Accuracy as function of time (III)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function that cuts data to specific time size but keeps same preprocessing\n",
    "    \n",
    "def preprocess_data(X_train_valid, y_train_valid, X_test,y_test, sub_sample, average,noise, trim_begin, trim_end, batch_size):\n",
    "    X_train_valid_prep,y_train_valid_prep = data_prep(X_train_valid,y_train_valid,sub_sample,average, noise, trim_begin, trim_end)\n",
    "    x_test, y_test = data_prep(X_test,y_test,sub_sample,average, noise, trim_begin, trim_end)\n",
    "\n",
    "    ## Random splitting and reshaping the data\n",
    "\n",
    "    # First generating the training and validation indices using random splitting\n",
    "    ind_valid = np.random.choice(8460, 5000, replace=False)\n",
    "    ind_train = np.array(list(set(range(8460)).difference(set(ind_valid))))\n",
    "\n",
    "    # Creating the training and validation sets using the generated indices\n",
    "    (x_train, x_valid) = X_train_valid_prep[ind_train], X_train_valid_prep[ind_valid] \n",
    "    (y_train, y_valid) = y_train_valid_prep[ind_train], y_train_valid_prep[ind_valid]\n",
    "\n",
    "    # change to tensor\n",
    "    x_train = Variable(torch.Tensor(x_train))\n",
    "    x_valid = Variable(torch.Tensor(x_valid))\n",
    "    x_test = Variable(torch.Tensor(x_test))\n",
    "\n",
    "    y_train = Variable(torch.Tensor(y_train))\n",
    "    y_train = torch.reshape(y_train,  (y_train.shape[0], 1)) \n",
    "\n",
    "    y_valid = Variable(torch.Tensor(y_valid))\n",
    "    y_valid = torch.reshape(y_valid,  (y_valid.shape[0], 1)) \n",
    "\n",
    "    y_test = Variable(torch.Tensor(y_test))\n",
    "    y_test = torch.reshape(y_test, (y_test.shape[0], 1))\n",
    "    print(\"\")\n",
    "    print('final shape')\n",
    "    print(\"Training Shape\", x_train.shape, y_train.shape)\n",
    "    print(\"Valid Shape\", x_valid.shape, y_valid.shape)\n",
    "    print(\"Test Shape\", x_test.shape, y_test.shape)\n",
    "    \n",
    "    train_dataset = torch.utils.data.TensorDataset(x_train, y_train)\n",
    "    valid_dataset = torch.utils.data.TensorDataset(x_valid, y_valid)\n",
    "    test_dataset  = torch.utils.data.TensorDataset(x_test, y_test)\n",
    "    \n",
    "    trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "    validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "    testloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "    return trainloader, validloader, testloader, x_train\n",
    "    \n",
    "testing = preprocess_data(static_X_train_valid, static_y_train_valid, static_X_test,static_y_test, 2, 2,True, 0, 500, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20 #epochs \n",
    "learning_rate = 0.001 #0.001 lr\n",
    "\n",
    "input_size = 250 #number of features \n",
    "hidden_size = 128 #number of features in hidden state\n",
    "num_layers = 1 #number of stacked lstm layers \n",
    "\n",
    "num_classes = 4 #number of output classes \n",
    "batch_size = 64 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "timesteps_test = [100,200,300,400,500,600,700,800,900,1000]\n",
    "val_accuracies = {}\n",
    "test_accuracies = {}\n",
    "losses = []\n",
    "\n",
    "for timestep in timesteps_test:\n",
    "    trainloader, validloader, testloader, x_train = preprocess_data(static_X_train_valid, static_y_train_valid, static_X_test,static_y_test, 2, 2,True, 0, timestep, 64)\n",
    "    #define new model to fit data\n",
    "    lstm1 = RNN(num_classes, timestep//2, hidden_size, num_layers, x_train.shape[1])\n",
    "    criterion = torch.nn.CrossEntropyLoss()   # CE Loss is our softmax\n",
    "    optimizer = torch.optim.Adam(lstm1.parameters(), lr=learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "\n",
    "    highest_val = 0\n",
    "    loss_array = []\n",
    "    for epoch in range(num_epochs):\n",
    "      for i, data in enumerate(trainloader,0):\n",
    "        inputs, labels = data\n",
    "        outputs = lstm1.forward(inputs) #forward pass\n",
    "        optimizer.zero_grad() #calculate the gradient, manually setting to 0\n",
    "        loss = criterion(outputs, labels.reshape(labels.size(0),).type(torch.long))\n",
    "        loss.backward() #calculates the loss of the loss function\n",
    "\n",
    "        optimizer.step() #improve from loss, i.e backprop\n",
    "        \n",
    "      scheduler.step()\n",
    "      print(\"Epoch: %d, loss: %1.5f\" % (epoch, loss.item()))\n",
    "      loss_array = np.append(loss_array, loss.item())  \n",
    "      correct = 0\n",
    "      total = 0\n",
    "      with torch.no_grad():\n",
    "        for data in validloader:\n",
    "            inputs, labels = data\n",
    "            outputs = lstm1(inputs)\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels.reshape(labels.size(0),)).sum().item()\n",
    "      print(\"  Val Accuracy: %1.5f\"% (float(correct) / float(total)))\n",
    "      if (float(correct) / float(total)) > highest_val:\n",
    "          highest_val = (float(correct) / float(total))\n",
    "    val_accuracies[timestep] = highest_val\n",
    "    #test set\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            inputs, labels = data\n",
    "            outputs = lstm1(inputs)\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels.reshape(labels.size(0),)).sum().item()\n",
    "        print(\"Test Accuracy: %1.5f\"% (float(correct) / float(total)))\n",
    "    test_accuracies[timestep] = (float(correct) / float(total))\n",
    "    losses += [loss_array]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Validation accuracies as function of time\", val_accuracies)\n",
    "print(\"\")\n",
    "print(\"Test accuracies as function of time\", test_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(2,1,1)\n",
    "plt.plot(val_accuracies.values())\n",
    "plt.plot(test_accuracies.values())\n",
    "plt.xlabel('Time x 100 cutoff')\n",
    "plt.ylabel('Model accuracy')\n",
    "plt.legend([\"val_acc\", \"test_acc\"])\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss Curves')\n",
    "plt.legend([\"100\", \"200\", \"300\", \"400\", \"500\", \"600\", \"700\", \"800\", \"900\", \"1000\"])\n",
    "# plt.plot(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0-700 timesteps had the highest classification accuracy on test data and 0-1000 timesteps had highest classification accuracy on validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing Classification across all subjects\n",
    "\n",
    "In this section we will be tuning the hyperparameters of our nerual nets across all subjects. We will examine any interesting trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50 #epochs \n",
    "learning_rate = 0.001 #0.0001 lr for LSTM only\n",
    "\n",
    "input_size = 250 #number of features \n",
    "hidden_size = 128 #number of features in hidden state\n",
    "num_layers = 1 #number of stacked lstm layers \n",
    "\n",
    "num_classes = 4 #number of output classes \n",
    "batch_size = 64 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Torch Itterables\n",
    "train_dataset = torch.utils.data.TensorDataset(x_train, y_train)\n",
    "valid_dataset = torch.utils.data.TensorDataset(x_valid, y_valid)\n",
    "test_dataset  = torch.utils.data.TensorDataset(x_test, y_test)\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Filtered Torch Itterables\n",
    "filtered_train_dataset = torch.utils.data.TensorDataset(x_train_den, y_train_den)\n",
    "filtered_valid_dataset = torch.utils.data.TensorDataset(x_valid_den, y_valid_den)\n",
    "filtered_test_dataset  = torch.utils.data.TensorDataset(x_test_den, y_test_den)\n",
    "filtered_trainloader = torch.utils.data.DataLoader(filtered_train_dataset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "filtered_validloader = torch.utils.data.DataLoader(filtered_valid_dataset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "filtered_testloader = torch.utils.data.DataLoader(filtered_test_dataset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CNN, classifier, and optimizer\n",
    "cnn_only = CNN(num_classes, input_size, hidden_size, num_layers, x_train.shape[1])\n",
    "cnn_criterion = torch.nn.CrossEntropyLoss()   # CE Loss is our softmax\n",
    "cnn_optimizer = torch.optim.Adam(cnn_only.parameters(), lr=learning_rate)\n",
    "cnn_scheduler = torch.optim.lr_scheduler.ExponentialLR(cnn_optimizer, gamma=0.9)\n",
    "\n",
    "# Create CNN, classifier, and optimizer for filtered data comparrison\n",
    "cnn_only_filt = CNN(num_classes, input_size, hidden_size, num_layers, x_train.shape[1])\n",
    "cnn_filt_criterion = torch.nn.CrossEntropyLoss()   # CE Loss is our softmax\n",
    "cnn_filt_optimizer = torch.optim.Adam(cnn_only_filt.parameters(), lr=learning_rate)\n",
    "cnn_filt_scheduler = torch.optim.lr_scheduler.ExponentialLR(cnn_filt_optimizer, gamma=0.9)\n",
    "\n",
    "# Create RNN, classifier, and optimizer\n",
    "rnn_only = RNN(num_classes, input_size, hidden_size, num_layers, x_train.shape[1])\n",
    "rnn_criterion = torch.nn.CrossEntropyLoss()   # CE Loss is our softmax\n",
    "rnn_optimizer = torch.optim.Adam(rnn_only.parameters(), lr=learning_rate)\n",
    "rnn_scheduler = torch.optim.lr_scheduler.ExponentialLR(rnn_optimizer, gamma=0.9)\n",
    "\n",
    "# Create RNN, classifier, and optimizer for filtered data comparrison\n",
    "rnn_only_filt = RNN(num_classes, input_size, hidden_size, num_layers, x_train.shape[1])\n",
    "rnn_filt_criterion = torch.nn.CrossEntropyLoss()   # CE Loss is our softmax\n",
    "rnn_filt_optimizer = torch.optim.Adam(rnn_only_filt.parameters(), lr=learning_rate)\n",
    "rnn_filt_scheduler = torch.optim.lr_scheduler.ExponentialLR(rnn_filt_optimizer, gamma=0.9)\n",
    "\n",
    "# Create CNN-LSTM, classifier, and optimizer\n",
    "cnn_lstm = CNN_LSTM(num_classes, input_size, hidden_size, num_layers, x_train.shape[1])\n",
    "cnn_lstm_criterion = torch.nn.CrossEntropyLoss()   # CE Loss is our softmax\n",
    "cnn_lstm_optimizer = torch.optim.Adam(cnn_lstm.parameters(), lr=learning_rate)\n",
    "cnn_lstm_scheduler = torch.optim.lr_scheduler.ExponentialLR(cnn_lstm_optimizer, gamma=0.9)\n",
    "\n",
    "# Create CNN-LSTM, classifier, and optimizer for filtered data comparrison\n",
    "cnn_lstm_filt = CNN_LSTM(num_classes, input_size, hidden_size, num_layers, x_train.shape[1])\n",
    "cnn_lstm_filt_criterion = torch.nn.CrossEntropyLoss()   # CE Loss is our softmax\n",
    "cnn_lstm_filt_optimizer = torch.optim.Adam(cnn_lstm_filt.parameters(), lr=learning_rate)\n",
    "cnn_lstm_filt_scheduler = torch.optim.lr_scheduler.ExponentialLR(cnn_lstm_filt_optimizer, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train CNN \n",
    "cnn_solver = Solver(num_epochs, cnn_only,cnn_optimizer, cnn_scheduler, cnn_criterion, verbose=False )\n",
    "cnn_solver.train(trainloader, validloader)\n",
    "print(\"CNN Training Done!\")\n",
    "\n",
    "# Train RNN\n",
    "rnn_solver = Solver(num_epochs, rnn_only,rnn_optimizer, rnn_scheduler, rnn_criterion, verbose=False, cnn_reshape=False )\n",
    "rnn_solver.train(trainloader, validloader)\n",
    "print(\"RNN Training Done!\")\n",
    "\n",
    "# Train CNN-LSTM\n",
    "cnn_lstm_solver = Solver(num_epochs, cnn_lstm,cnn_lstm_optimizer, cnn_lstm_scheduler, cnn_lstm_criterion, verbose=False )\n",
    "cnn_lstm_solver.train(trainloader, validloader)\n",
    "print(\"CNN-LSTM Training Done!\")\n",
    "\n",
    "# Train CNN on filtered Data\n",
    "cnn_filt_solver = Solver(num_epochs, cnn_only_filt,cnn_filt_optimizer, cnn_filt_scheduler, cnn_filt_criterion, verbose=False )\n",
    "cnn_filt_solver.train(filtered_trainloader, filtered_validloader)\n",
    "print(\"CNN Training on Filtered Data Done!\")\n",
    "\n",
    "# Train RNN on filtered Data\n",
    "rnn_filt_solver = Solver(num_epochs, rnn_only_filt,rnn_filt_optimizer, rnn_filt_scheduler, rnn_filt_criterion, verbose=False, cnn_reshape=False )\n",
    "rnn_filt_solver.train(filtered_trainloader, filtered_validloader)\n",
    "print(\"RNN Training on Filtered Data Done!\")\n",
    "\n",
    "# Train CNN-LSTM on filtered Data\n",
    "cnn_lstm_filt_solver = Solver(num_epochs, cnn_lstm_filt,cnn_lstm_filt_optimizer, cnn_lstm_filt_scheduler, cnn_lstm_filt_criterion, verbose=False )\n",
    "cnn_lstm_filt_solver.train(filtered_trainloader, filtered_validloader)\n",
    "print(\"CNN-LSTM Training on Filtered Data Done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot CNN Results\n",
    "fig, axs = plt.subplots(2)\n",
    "fig.suptitle('CNN Training Breakdown')\n",
    "axs[0].plot(cnn_solver.loss_history)\n",
    "axs[0].plot(cnn_solver.val_loss_history)\n",
    "axs[0].plot(cnn_filt_solver.loss_history)\n",
    "axs[0].plot(cnn_filt_solver.val_loss_history)\n",
    "axs[0].legend([\"Unfiltered train\", \"Unfiltered val\", \"Filtered Train\", \"Filtered Validation\"])\n",
    "axs[0].set_ylabel(\"Loss\")\n",
    "axs[0].set_xlabel(\"Epoch\")\n",
    "axs[0].set_title(\"CNN Loss Curves\")\n",
    "axs[1].plot(cnn_solver.val_accuracy_history)\n",
    "axs[1].plot(cnn_filt_solver.val_accuracy_history)\n",
    "axs[1].legend([\"Unfiltered Validation Accuracy\", \"Filtered Validation Accuracy\"])\n",
    "axs[1].set_ylabel(\"Accuracy\")\n",
    "axs[1].set_xlabel(\"Epoch\")\n",
    "axs[1].set_title(\"CNN Accuracy Over Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot RNN Results\n",
    "fig, axs = plt.subplots(2)\n",
    "fig.suptitle('RNN Training Breakdown')\n",
    "axs[0].plot(rnn_solver.loss_history)\n",
    "axs[0].plot(rnn_solver.val_loss_history)\n",
    "axs[0].plot(rnn_filt_solver.loss_history)\n",
    "axs[0].plot(rnn_filt_solver.val_loss_history)\n",
    "axs[0].legend([\"Unfiltered train\", \"Unfiltered val\", \"Filtered Train\", \"Filtered Validation\"])\n",
    "axs[0].set_ylabel(\"Loss\")\n",
    "axs[0].set_xlabel(\"Epoch\")\n",
    "axs[0].set_title(\"RNN Loss Curves\")\n",
    "axs[1].plot(rnn_solver.val_accuracy_history)\n",
    "axs[1].plot(rnn_filt_solver.val_accuracy_history)\n",
    "axs[1].legend([\"Unfiltered Validation Accuracy\", \"Filtered Validation Accuracy\"])\n",
    "axs[1].set_ylabel(\"Accuracy\")\n",
    "axs[1].set_xlabel(\"Epoch\")\n",
    "axs[1].set_title(\"RNN Accuracy Over Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot CNN-LSTM Results\n",
    "fig, axs = plt.subplots(2)\n",
    "fig.suptitle('CNN-LSTM Training Breakdown')\n",
    "axs[0].plot(cnn_lstm_solver.loss_history)\n",
    "axs[0].plot(cnn_lstm_solver.val_loss_history)\n",
    "axs[0].plot(cnn_lstm_filt_solver.loss_history)\n",
    "axs[0].plot(cnn_lstm_filt_solver.val_loss_history)\n",
    "axs[0].legend([\"Unfiltered train\", \"Unfiltered val\", \"Filtered Train\", \"Filtered Validation\"])\n",
    "axs[0].set_ylabel(\"Loss\")\n",
    "axs[0].set_xlabel(\"Epoch\")\n",
    "axs[0].set_title(\"CNN-LSTM Loss Curves\")\n",
    "axs[1].plot(cnn_lstm_solver.val_accuracy_history)\n",
    "axs[1].plot(cnn_lstm_filt_solver.val_accuracy_history)\n",
    "axs[1].legend([\"Unfiltered Validation Accuracy\", \"Filtered Validation Accuracy\"])\n",
    "axs[1].set_ylabel(\"Accuracy\")\n",
    "axs[1].set_xlabel(\"Epoch\")\n",
    "axs[1].set_title(\"CNN-LSTM Accuracy Over Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_accuracy = cnn_solver.test(testloader)\n",
    "print(\"CNN Test Accuracy: %1.5f\"% (cnn_accuracy))\n",
    "cnn_filt_accuracy = cnn_filt_solver.test(filtered_testloader)\n",
    "print(\"CNN Test Accuracy(Filtered): %1.5f\"% (cnn_filt_accuracy))\n",
    "rnn_accuracy = rnn_solver.test(testloader)\n",
    "print(\"RNN Test Accuracy: %1.5f\"% (rnn_accuracy))\n",
    "rnn_filt_accuracy = rnn_filt_solver.test(filtered_testloader)\n",
    "print(\"RNN Test Accuracy(Filtered): %1.5f\"% (rnn_filt_accuracy))\n",
    "cnn_lstm_accuracy = cnn_lstm_solver.test(testloader)\n",
    "print(\"CNN-LSTM Test Accuracy: %1.5f\"% (cnn_lstm_accuracy))\n",
    "cnn_lstm_filt_accuracy = cnn_lstm_filt_solver.test(filtered_testloader)\n",
    "print(\"CNN-LSTM Test Accuracy(Filtered): %1.5f\"% (cnn_lstm_filt_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN Test Accuracy Breakdown\n",
    "totalsub=9\n",
    "CNN_accuracy_by_class = cnn_solver.test_by_class(testloader)\n",
    "CNN_accuracy_by_patient = np.zeros(totalsub)\n",
    "\n",
    "# Patient Accuracies\n",
    "for sub in range(totalsub):\n",
    "    # Partition Test set by patient number\n",
    "    sub_test_dataset  = torch.utils.data.TensorDataset(x_test[np.where(person_test == sub)[0]], y_test[np.where(person_test == sub)[0]])\n",
    "    sub_testloader = torch.utils.data.DataLoader(sub_test_dataset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "    CNN_accuracy_by_patient[sub] = cnn_solver.test(sub_testloader)\n",
    "fig, axs = plt.subplots(2)\n",
    "fig.suptitle('CNN Accuracy Breakdown')\n",
    "axs[0].scatter(np.arange(num_classes),CNN_accuracy_by_class)\n",
    "axs[0].set_ylabel(\"Accuracy\")\n",
    "axs[0].set_xlabel(\"Class\")\n",
    "axs[1].scatter(np.arange(totalsub),CNN_accuracy_by_patient)\n",
    "axs[1].set_ylabel(\"Accuracy\")\n",
    "axs[1].set_xlabel(\"Subject Index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN Test Accuracy Breakdown\n",
    "totalsub=9\n",
    "RNN_accuracy_by_class = rnn_solver.test_by_class(testloader)\n",
    "RNN_accuracy_by_patient = np.zeros(totalsub)\n",
    "\n",
    "# Patient Accuracies\n",
    "for sub in range(totalsub):\n",
    "    # Partition Test set by patient number\n",
    "    sub_test_dataset  = torch.utils.data.TensorDataset(x_test[np.where(person_test == sub)[0]], y_test[np.where(person_test == sub)[0]])\n",
    "    sub_testloader = torch.utils.data.DataLoader(sub_test_dataset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "    RNN_accuracy_by_patient[sub] = rnn_solver.test(sub_testloader)\n",
    "fig, axs = plt.subplots(2)\n",
    "fig.suptitle('LSTM Accuracy Breakdown')\n",
    "\n",
    "axs[0].scatter(np.arange(num_classes),RNN_accuracy_by_class)\n",
    "axs[0].set_ylabel(\"Accuracy\")\n",
    "axs[0].set_xlabel(\"Class\")\n",
    "axs[1].scatter(np.arange(totalsub),RNN_accuracy_by_patient)\n",
    "axs[1].set_ylabel(\"Accuracy\")\n",
    "axs[1].set_xlabel(\"Subject Index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN-LSTM Test Accuracy Breakdown\n",
    "totalsub=9\n",
    "CNN_LSTM_accuracy_by_class = cnn_lstm_solver.test_by_class(testloader)\n",
    "CNN_LSTM_accuracy_by_patient = np.zeros(totalsub)\n",
    "\n",
    "# Patient Accuracies\n",
    "for sub in range(totalsub):\n",
    "    # Partition Test set by patient number\n",
    "    sub_test_dataset  = torch.utils.data.TensorDataset(x_test[np.where(person_test == sub)[0]], y_test[np.where(person_test == sub)[0]])\n",
    "    sub_testloader = torch.utils.data.DataLoader(sub_test_dataset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "    CNN_LSTM_accuracy_by_patient[sub] = cnn_lstm_solver.test(sub_testloader)\n",
    "fig, axs = plt.subplots(2)\n",
    "fig.suptitle('CNN-LSTM Accuracy Breakdown')\n",
    "axs[0].scatter(np.arange(num_classes),CNN_LSTM_accuracy_by_class)\n",
    "axs[0].set_ylabel(\"Accuracy\")\n",
    "axs[0].set_xlabel(\"Class\")\n",
    "axs[1].scatter(np.arange(totalsub),CNN_LSTM_accuracy_by_patient)\n",
    "axs[1].set_ylabel(\"Accuracy\")\n",
    "axs[1].set_xlabel(\"Subject Index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3)\n",
    "fig.suptitle('Loss Breakdown')\n",
    "axs[2].plot(cnn_lstm_solver.loss_history)\n",
    "axs[2].plot(cnn_lstm_solver.val_loss_history)\n",
    "axs[2].plot(cnn_lstm_filt_solver.loss_history)\n",
    "axs[2].plot(cnn_lstm_filt_solver.val_loss_history)\n",
    "axs[2].legend([\"Unfiltered train\", \"Unfiltered val\", \"Filtered Train\", \"Filtered Validation\"])\n",
    "axs[2].set_ylabel(\"Loss\")\n",
    "axs[2].set_xlabel(\"Epoch\")\n",
    "axs[2].set_title(\"CNN-LSTM Loss Curves\")\n",
    "\n",
    "# Plot RNN Results\n",
    "axs[1].plot(rnn_solver.loss_history)\n",
    "axs[1].plot(rnn_solver.val_loss_history)\n",
    "axs[1].plot(rnn_filt_solver.loss_history)\n",
    "axs[1].plot(rnn_filt_solver.val_loss_history)\n",
    "axs[1].legend([\"Unfiltered train\", \"Unfiltered val\", \"Filtered Train\", \"Filtered Validation\"])\n",
    "axs[1].set_ylabel(\"Loss\")\n",
    "axs[1].set_xlabel(\"Epoch\")\n",
    "axs[1].set_title(\"RNN Loss Curves\")\n",
    "\n",
    "# Plot CNN Results\n",
    "axs[0].plot(cnn_solver.loss_history)\n",
    "axs[0].plot(cnn_solver.val_loss_history)\n",
    "axs[0].plot(cnn_filt_solver.loss_history)\n",
    "axs[0].plot(cnn_filt_solver.val_loss_history)\n",
    "axs[0].legend([\"Unfiltered train\", \"Unfiltered val\", \"Filtered Train\", \"Filtered Validation\"])\n",
    "axs[0].set_ylabel(\"Loss\")\n",
    "axs[0].set_xlabel(\"Epoch\")\n",
    "axs[0].set_title(\"CNN Loss Curves\")\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot CNN Results\n",
    "fig, axs = plt.subplots(3)\n",
    "fig.suptitle('Validation Accuracy Breakdown')\n",
    "\n",
    "axs[0].plot(cnn_solver.val_accuracy_history)\n",
    "axs[0].plot(cnn_filt_solver.val_accuracy_history)\n",
    "axs[0].legend([\"Unfiltered Validation Accuracy\", \"Filtered Validation Accuracy\"])\n",
    "axs[0].set_ylabel(\"Accuracy\")\n",
    "axs[0].set_xlabel(\"Epoch\")\n",
    "axs[0].set_title(\"CNN ValidationAccuracy\")\n",
    "\n",
    "# Plot RNN Results\n",
    "\n",
    "axs[1].plot(rnn_solver.val_accuracy_history)\n",
    "axs[1].plot(rnn_filt_solver.val_accuracy_history)\n",
    "axs[1].legend([\"Unfiltered Validation Accuracy\", \"Filtered Validation Accuracy\"])\n",
    "axs[1].set_ylabel(\"Accuracy\")\n",
    "axs[1].set_xlabel(\"Epoch\")\n",
    "axs[1].set_title(\"RNN Validation Accuracy\")\n",
    "\n",
    "# Plot CNN-LSTM Results\n",
    "\n",
    "axs[2].plot(cnn_lstm_solver.val_accuracy_history)\n",
    "axs[2].plot(cnn_lstm_filt_solver.val_accuracy_history)\n",
    "axs[2].legend([\"Unfiltered Validation Accuracy\", \"Filtered Validation Accuracy\"])\n",
    "axs[2].set_ylabel(\"Accuracy\")\n",
    "axs[2].set_xlabel(\"Epoch\")\n",
    "axs[2].set_title(\"CNN-LSTM Validation Accuracy\")\n",
    "\n",
    "print(\"CNN Max Validation Accuracy:           %1.5f\"% (np.max(cnn_solver.val_accuracy_history)))\n",
    "print(\"CNN Max Validation Accuracy(Filtered): %1.5f\"% (np.max(cnn_filt_solver.val_accuracy_history)))\n",
    "print(\"RNN Max Validation Accuracy:           %1.5f\"% (np.max(rnn_solver.val_accuracy_history)))\n",
    "print(\"RNN Max Validation Accuracy(Filtered): %1.5f\"% (np.max(rnn_filt_solver.val_accuracy_history)))\n",
    "print(\"CNN-LSTM Max Validation Accuracy          : %1.5f\"% (np.max(cnn_lstm_solver.val_accuracy_history)))\n",
    "print(\"CNN-LSTM Max Validation Accuracy(Filtered): %1.5f\"% (np.max(cnn_lstm_filt_solver.val_accuracy_history)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare CNN to CNN-LSTM\n",
    "cnn_accs = []\n",
    "cnn_lstm_accs = []\n",
    "for i in range(10):\n",
    "    all_subjects_net_cnn = CNN(num_classes, input_size, hidden_size, num_layers, x_train.shape[1])\n",
    "    all_subjects_net = CNN_LSTM(num_classes, input_size, hidden_size, num_layers, x_train.shape[1])\n",
    "    criterion = torch.nn.CrossEntropyLoss()   # CE Loss is our softmax\n",
    "    optimizer = torch.optim.Adam(all_subjects_net.parameters(), lr=learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "    criterion1 = torch.nn.CrossEntropyLoss()   # CE Loss is our softmax\n",
    "    optimizer1 = torch.optim.Adam(all_subjects_net_cnn.parameters(), lr=learning_rate)\n",
    "    scheduler1 = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "    \n",
    "    solver = Solver(num_epochs, all_subjects_net,optimizer, scheduler, criterion, verbose=True )\n",
    "    solver.train(trainloader, validloader)\n",
    "    solver1 = Solver(num_epochs, all_subjects_net_cnn,optimizer1, scheduler1, criterion1, verbose=True )\n",
    "    solver1.train(trainloader, validloader)\n",
    "\n",
    "    cnn_accs.append(solver1.test(testloader))\n",
    "    cnn_lstm_accs.append(solver.test(testloader))\n",
    "\n",
    "print(np.mean(cnn_accs))\n",
    "print(np.mean(cnn_lstm_accs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    all_subjects_net_cnn = CNN(num_classes, input_size, hidden_size, num_layers, x_train.shape[1])\n",
    "    all_subjects_net = CNN_LSTM(num_classes, input_size, hidden_size, num_layers, x_train.shape[1])\n",
    "    criterion = torch.nn.CrossEntropyLoss()   # CE Loss is our softmax\n",
    "    optimizer = torch.optim.Adam(all_subjects_net.parameters(), lr=learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "    criterion1 = torch.nn.CrossEntropyLoss()   # CE Loss is our softmax\n",
    "    optimizer1 = torch.optim.Adam(all_subjects_net_cnn.parameters(), lr=learning_rate)\n",
    "    scheduler1 = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "    \n",
    "    solver = Solver(50, all_subjects_net,optimizer, scheduler, criterion, verbose=False )\n",
    "    solver.train(trainloader, validloader)\n",
    "    solver1 = Solver(50, all_subjects_net_cnn,optimizer1, scheduler1, criterion1, verbose=False )\n",
    "    solver1.train(trainloader, validloader)\n",
    "\n",
    "    cnn_acc = solver1.test(testloader)\n",
    "    cnn_lstm_acc = solver.test(testloader)\n",
    "    print(\"CNN Accuracy: %1.5f\"% (cnn_acc))\n",
    "    print(\"CNN_LSTM Accuracy: %1.5f\"% (cnn_lstm_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(solver.loss_history)\n",
    "plt.plot(solver.val_loss_history)\n",
    "plt.legend([\"train\", \"val\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(solver1.loss_history)\n",
    "plt.plot(solver1.val_loss_history)\n",
    "plt.legend([\"train\", \"val\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7783c1c403da32ab0836993f6933693559c492907b6c924b33c48fab4351724d"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
